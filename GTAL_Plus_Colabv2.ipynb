{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169359f4",
   "metadata": {},
   "source": [
    "# GTAL+ : Game-Theoretic Attention Learning for Fine-Grained Classification\n",
    "\n",
    "### Key Features:\n",
    "- âœ… **GTAL Core**: Game-theoretic attention with Nash Equilibrium\n",
    "- âœ… **4 Players**: Early, Mid, Late, Semantic feature levels\n",
    "- âœ… **448Ã—448 Images**: Critical for fine-grained recognition\n",
    "- âœ… **SGD + StepLR**: Proven training configuration\n",
    "\n",
    "### Configuration Comparison:\n",
    "| Parameter | GTAL+ |\n",
    "|-----------|--------------|\n",
    "| Image Size  | **448Ã—448** |\n",
    "| Loss | **CrossEntropy** |\n",
    "| Optimizer | **SGD** |\n",
    "| LR Schedule | **StepLR** |\n",
    "| Epochs | **95** |\n",
    "| Batch Size | **64** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1. SETUP & IMPORTS\n",
    "# =====================================================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# GOOGLE COLAB SETUP\n",
    "# ==========================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ==========================================\n",
    "# PATHS\n",
    "# ==========================================\n",
    "BASE_PATH = \"/content/drive/My Drive/Project_GTAL\"\n",
    "TRAIN_CSV = os.path.join(BASE_PATH, \"image_list_TRAIN.csv\")\n",
    "TEST_CSV = os.path.join(BASE_PATH, \"image_list_TEST.csv\")\n",
    "\n",
    "# VERSION 3 - New approach\n",
    "CHECKPOINT_DIR = \"./checkpoints/gtal_plus_v3\"\n",
    "VIZ_DIR = \"./visualizations/gtal_plus_v3\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(VIZ_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# DEVICE SETUP\n",
    "# ==========================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'âœ“ Device: {device}')\n",
    "print(f'âœ“ CUDA Available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'âœ“ GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f'\\nâœ“ Paths configured:')\n",
    "print(f'  BASE_PATH: {BASE_PATH}')\n",
    "print(f'  CHECKPOINT_DIR: {CHECKPOINT_DIR} (v3)')\n",
    "print(f'  VIZ_DIR: {VIZ_DIR} (v3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4568e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2. CONFIGURATION \n",
    "# =====================================================\n",
    "\n",
    "class GTALPlusConfig:\n",
    "    \n",
    "    # Image settings\n",
    "    IMG_SIZE = 448\n",
    "    \n",
    "    # Training settings\n",
    "    EPOCHS = 95\n",
    "    BATCH_SIZE = 64\n",
    "    BASE_LR = 0.001\n",
    "    MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    \n",
    "    # LR Schedule\n",
    "    LR_STEP_SIZE = 30\n",
    "    LR_GAMMA = 0.1\n",
    "    \n",
    "    # Model settings\n",
    "    NUM_CLASSES = 200\n",
    "    EMBEDDING_DIM = 512\n",
    "    \n",
    "    # ========== V3 GTAL SETTINGS ==========\n",
    "    GTAL_ITERATIONS = 8           \n",
    "    GTAL_TEMPERATURE = 0.3        \n",
    "    GTAL_LEARNING_RATE = 0.2      \n",
    "    REDUNDANCY_PENALTY = 0.2      \n",
    "    \n",
    "    # ========== V3 LOSS SETTINGS ==========\n",
    "    DIVERSITY_WEIGHT = 0.05       \n",
    "    AUXILIARY_WEIGHT = 0.1        \n",
    "    LABEL_SMOOTHING = 0.1         \n",
    "    \n",
    " \n",
    "    WARMUP_EPOCHS = 10           \n",
    "    \n",
    "    # Ensemble prediction\n",
    "    USE_ENSEMBLE = True           \n",
    "\n",
    "config = GTALPlusConfig()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GTAL+ v3 BALANCED CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nImage: {config.IMG_SIZE}Ã—{config.IMG_SIZE}\")\n",
    "print(f\"\\nTraining: {config.EPOCHS} epochs, batch={config.BATCH_SIZE}, lr={config.BASE_LR}\")\n",
    "print(f\"\\nGTAL (Relaxed):\")\n",
    "print(f\"   Iterations: {config.GTAL_ITERATIONS}\")\n",
    "print(f\"   Temperature: {config.GTAL_TEMPERATURE}\")\n",
    "print(f\"   Redundancy Penalty: {config.REDUNDANCY_PENALTY} (â†“ from 0.5)\")\n",
    "print(f\"\\nV3 Improvements:\")\n",
    "print(f\"   Diversity Weight: {config.DIVERSITY_WEIGHT} (â†“â†“ from 0.3)\")\n",
    "print(f\"   Auxiliary Weight: {config.AUXILIARY_WEIGHT} (â†“â†“ from 0.4)\")\n",
    "print(f\"   Label Smoothing: {config.LABEL_SMOOTHING}\")\n",
    "print(f\"   Warmup Epochs: {config.WARMUP_EPOCHS}\")\n",
    "print(f\"   Ensemble Mode: {config.USE_ENSEMBLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9157521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3. DATA TRANSFORMS \n",
    "# =====================================================\n",
    "\n",
    "# Training transforms \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(config.IMG_SIZE),  # 448x448\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Test transforms \n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(int(config.IMG_SIZE / 0.875)),  # ~512 for 448\n",
    "    transforms.CenterCrop(config.IMG_SIZE),           # 448x448\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"âœ“ Data transforms defined (matching resnet_finetune_cub)\")\n",
    "print(f\"  Training: RandomResizedCrop({config.IMG_SIZE}) + RandomHorizontalFlip\")\n",
    "print(f\"  Testing: Resize({int(config.IMG_SIZE/0.875)}) + CenterCrop({config.IMG_SIZE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60677b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 4. DATASET CLASS\n",
    "# =====================================================\n",
    "\n",
    "class CUB200Dataset(Dataset):\n",
    "    \"\"\"CUB-200 Dataset loading from CSV files\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.labels = self.df['label'].values\n",
    "        self.paths = self.df['path'].values\n",
    "        \n",
    "        # Compute class statistics\n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        self.num_classes = len(unique)\n",
    "        self.class_counts = dict(zip(unique, counts))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, self.labels[idx]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.paths[idx]}: {e}\")\n",
    "            # Return blank image on error\n",
    "            img = torch.zeros(3, config.IMG_SIZE, config.IMG_SIZE)\n",
    "            return img, self.labels[idx]\n",
    "\n",
    "\n",
    "# Load and check datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = CUB200Dataset(TRAIN_CSV, transform=train_transform)\n",
    "test_dataset = CUB200Dataset(TEST_CSV, transform=test_transform)\n",
    "\n",
    "print(f\"\\nâœ“ Datasets loaded:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples, {train_dataset.num_classes} classes\")\n",
    "print(f\"  Test:  {len(test_dataset)} samples, {test_dataset.num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5902ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 5. GTAL CORE COMPONENTS \n",
    "# =====================================================\n",
    "\n",
    "class PayoffFunction(nn.Module):\n",
    "    \"\"\"Computes relevance score for each feature level\"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        # Initialize to output similar values\n",
    "        nn.init.xavier_uniform_(self.net[-1].weight, gain=0.1)\n",
    "        nn.init.zeros_(self.net[-1].bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GTALLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    V3 GTAL Layer - Simplified but effective\n",
    "    \n",
    "    Key changes:\n",
    "    - Simpler best response (no min constraint that hurt training)\n",
    "    - Learnable bias terms for each feature level\n",
    "    - More stable gradient flow\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dims=[256, 512, 1024, 2048], common_dim=256):\n",
    "        super().__init__()\n",
    "        self.num_players = len(feature_dims)\n",
    "        \n",
    "        # Payoff networks\n",
    "        self.payoff_nets = nn.ModuleList([\n",
    "            PayoffFunction(dim) for dim in feature_dims\n",
    "        ])\n",
    "        \n",
    "        # Learnable prior bias (helps balance initial weights)\n",
    "        # Initialize to encourage some diversity\n",
    "        self.prior_bias = nn.Parameter(torch.tensor([0.1, 0.15, 0.2, 0.55]))\n",
    "        \n",
    "        # Redundancy projection\n",
    "        self.projectors = nn.ModuleList([\n",
    "            nn.Linear(dim, common_dim) for dim in feature_dims\n",
    "        ])\n",
    "        \n",
    "        self.temperature = config.GTAL_TEMPERATURE\n",
    "        self.num_iterations = config.GTAL_ITERATIONS\n",
    "\n",
    "    def compute_redundancy(self, features):\n",
    "        \"\"\"Compute pairwise feature redundancy\"\"\"\n",
    "        batch_size = features[0].size(0)\n",
    "        n = len(features)\n",
    "        \n",
    "        # Project and normalize\n",
    "        proj = [F.normalize(self.projectors[i](features[i]), dim=1) for i in range(n)]\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        redundancy = torch.zeros(batch_size, n, n, device=features[0].device)\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    sim = (proj[i] * proj[j]).sum(dim=1)\n",
    "                    redundancy[:, i, j] = sim.abs()\n",
    "        return redundancy\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = features[0].size(0)\n",
    "        \n",
    "        # Compute payoffs\n",
    "        payoffs = torch.cat([\n",
    "            self.payoff_nets[i](features[i]) for i in range(self.num_players)\n",
    "        ], dim=1)  # (B, 4)\n",
    "        \n",
    "        # Add learnable prior\n",
    "        payoffs = payoffs + self.prior_bias.unsqueeze(0)\n",
    "        \n",
    "        # Compute redundancy\n",
    "        redundancy = self.compute_redundancy(features)\n",
    "        \n",
    "        # Initialize weights\n",
    "        weights = F.softmax(self.prior_bias.unsqueeze(0).expand(batch_size, -1), dim=1)\n",
    "        \n",
    "        # Best response iterations\n",
    "        for _ in range(self.num_iterations):\n",
    "            # Redundancy penalty\n",
    "            penalty = torch.bmm(redundancy, weights.unsqueeze(-1)).squeeze(-1)\n",
    "            \n",
    "            # Effective payoff\n",
    "            effective = payoffs - config.REDUNDANCY_PENALTY * penalty\n",
    "            \n",
    "            # Best response\n",
    "            best_response = F.softmax(effective / self.temperature, dim=1)\n",
    "            \n",
    "            # Smooth update\n",
    "            weights = 0.7 * weights + 0.3 * best_response\n",
    "        \n",
    "        return weights\n",
    "\n",
    "\n",
    "def soft_diversity_loss(weights):\n",
    "\n",
    "    max_weight = weights.max(dim=1)[0]\n",
    "    # Only penalize if max > 0.8\n",
    "    penalty = F.relu(max_weight - 0.8)\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "print(\"âœ“ V3 GTAL Components:\")\n",
    "print(\"  - Simplified PayoffFunction\")\n",
    "print(\"  - GTALLayer with learnable prior bias\")\n",
    "print(\"  - Soft diversity loss (only extreme imbalance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc846c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 6. GTAL+ MODEL\n",
    "# =====================================================\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        s = torch.sigmoid(self.fc2(F.relu(self.fc1(x))))\n",
    "        return x * s\n",
    "\n",
    "\n",
    "class GTALPlusModelV3(nn.Module):\n",
    "    \"\"\"\n",
    "    GTAL+ V3 Model - Ensemble Strategy\n",
    "    \n",
    "    \n",
    "    Strategy:\n",
    "    1. Main classifier on semantic features\n",
    "    2. GTAL fusion as enhancement\n",
    "    3. Ensemble both predictions\n",
    "    \n",
    "    This way we get:\n",
    "    - Baseline-level performance from semantic path\n",
    "    - Potential boost from multi-scale GTAL fusion\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=200, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load pretrained ResNet-50\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "        # Feature stages\n",
    "        self.early = nn.Sequential(\n",
    "            resnet.conv1, resnet.bn1, resnet.relu,\n",
    "            resnet.maxpool, resnet.layer1\n",
    "        )\n",
    "        self.mid = resnet.layer2\n",
    "        self.late = resnet.layer3\n",
    "        self.semantic = resnet.layer4\n",
    "        \n",
    "        # Feature projections with SE\n",
    "        self.proj_early = nn.Sequential(\n",
    "            nn.Linear(256, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.proj_mid = nn.Sequential(\n",
    "            nn.Linear(512, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.proj_late = nn.Sequential(\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.proj_semantic = nn.Sequential(\n",
    "            nn.Linear(2048, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.se_early = SEBlock(embedding_dim)\n",
    "        self.se_mid = SEBlock(embedding_dim)\n",
    "        self.se_late = SEBlock(embedding_dim)\n",
    "        self.se_semantic = SEBlock(embedding_dim)\n",
    "        \n",
    "        # GTAL Layer\n",
    "        self.gtal = GTALLayer(\n",
    "            feature_dims=[256, 512, 1024, 2048],\n",
    "            common_dim=256\n",
    "        )\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Path 1: Direct semantic classifier (like baseline)\n",
    "        self.semantic_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "        # Initialize like baseline\n",
    "        nn.init.kaiming_normal_(self.semantic_classifier[1].weight)\n",
    "        nn.init.zeros_(self.semantic_classifier[1].bias)\n",
    "        \n",
    "        # Path 2: GTAL fusion classifier\n",
    "        self.gtal_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(embedding_dim, num_classes)\n",
    "        )\n",
    "        nn.init.kaiming_normal_(self.gtal_classifier[1].weight)\n",
    "        nn.init.zeros_(self.gtal_classifier[1].bias)\n",
    "        \n",
    "        # Learnable ensemble weight (starts at 0.5/0.5)\n",
    "        self.ensemble_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "        self.feature_weights = None\n",
    "\n",
    "    def forward(self, x, return_both=False):\n",
    "        # Extract features\n",
    "        e = self.early(x)\n",
    "        m = self.mid(e)\n",
    "        l = self.late(m)\n",
    "        s = self.semantic(l)\n",
    "        \n",
    "        # Global average pooling\n",
    "        p_e = F.adaptive_avg_pool2d(e, 1).flatten(1)\n",
    "        p_m = F.adaptive_avg_pool2d(m, 1).flatten(1)\n",
    "        p_l = F.adaptive_avg_pool2d(l, 1).flatten(1)\n",
    "        p_s = F.adaptive_avg_pool2d(s, 1).flatten(1)\n",
    "        \n",
    "        # ===== Path 1: Direct semantic (baseline-like) =====\n",
    "        semantic_logits = self.semantic_classifier(p_s)\n",
    "        \n",
    "        # ===== Path 2: GTAL fusion =====\n",
    "        weights = self.gtal([p_e, p_m, p_l, p_s])\n",
    "        self.feature_weights = weights.detach()\n",
    "        \n",
    "        # Project and apply SE\n",
    "        f_e = self.se_early(self.proj_early(p_e))\n",
    "        f_m = self.se_mid(self.proj_mid(p_m))\n",
    "        f_l = self.se_late(self.proj_late(p_l))\n",
    "        f_s = self.se_semantic(self.proj_semantic(p_s))\n",
    "        \n",
    "        # Weighted fusion\n",
    "        fused = (weights[:, 0:1] * f_e + \n",
    "                 weights[:, 1:2] * f_m + \n",
    "                 weights[:, 2:3] * f_l + \n",
    "                 weights[:, 3:4] * f_s)\n",
    "        \n",
    "        gtal_logits = self.gtal_classifier(fused)\n",
    "        \n",
    "        # ===== Ensemble =====\n",
    "        alpha = torch.sigmoid(self.ensemble_weight)  # Constrain to [0,1]\n",
    "        ensemble_logits = alpha * semantic_logits + (1 - alpha) * gtal_logits\n",
    "        \n",
    "        if return_both:\n",
    "            return ensemble_logits, weights, semantic_logits, gtal_logits\n",
    "        \n",
    "        return ensemble_logits, weights\n",
    "\n",
    "\n",
    "# Test the model\n",
    "print(\"Testing GTAL+ V3 Model...\")\n",
    "model = GTALPlusModelV3(num_classes=200).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nâœ“ GTAL+ V3 Model created:\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "\n",
    "test_input = torch.randn(2, 3, config.IMG_SIZE, config.IMG_SIZE).to(device)\n",
    "with torch.no_grad():\n",
    "    out, w, sem, gtal = model(test_input, return_both=True)\n",
    "    alpha = torch.sigmoid(model.ensemble_weight).item()\n",
    "    \n",
    "print(f\"\\nâœ“ Forward pass test:\")\n",
    "print(f\"  Output: {out.shape}\")\n",
    "print(f\"  GTAL weights: {w[0].cpu().numpy().round(3)}\")\n",
    "print(f\"  Ensemble Î±: {alpha:.3f} (semantic) / {1-alpha:.3f} (gtal)\")\n",
    "\n",
    "del test_input, out, w, sem, gtal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30aeac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 7. TRAINING FUNCTION \n",
    "# =====================================================\n",
    "\n",
    "def train_gtal_plus_v3(model, train_loader, test_loader, epochs=None):\n",
    "    if epochs is None:\n",
    "        epochs = config.EPOCHS\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss with label smoothing\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=config.LABEL_SMOOTHING)\n",
    "    \n",
    "    # SGD optimizer\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.BASE_LR,\n",
    "        momentum=config.MOMENTUM,\n",
    "        weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # StepLR scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=config.LR_STEP_SIZE,\n",
    "        gamma=config.LR_GAMMA\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'test_acc': [],\n",
    "        'feature_weights': [], 'lr': [], 'ensemble_alpha': [],\n",
    "        'semantic_acc': [], 'gtal_acc': []\n",
    "    }\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"GTAL+ V3 TRAINING - Ensemble Approach\")\n",
    "    print(\"=\"*85)\n",
    "    print(f\"Strategy: Semantic path (baseline) + GTAL path â†’ Ensemble\")\n",
    "    print(f\"Label Smoothing: {config.LABEL_SMOOTHING}\")\n",
    "    print(f\"Warmup: {config.WARMUP_EPOCHS} epochs (no extra losses)\")\n",
    "    print(\"-\"*85)\n",
    "    print(\"Epoch\\tLoss\\tTrain%\\tTest%\\tSem%\\tGTAL%\\tÎ±\\tWeights\")\n",
    "    print(\"-\"*85)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        epoch_weights = []\n",
    "        \n",
    "        # Check if in warmup\n",
    "        in_warmup = epoch < config.WARMUP_EPOCHS\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
    "        for imgs, labels in pbar:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            ensemble_logits, weights, sem_logits, gtal_logits = model(imgs, return_both=True)\n",
    "            \n",
    "            # Main loss on ensemble\n",
    "            main_loss = criterion(ensemble_logits, labels)\n",
    "            \n",
    "            # Auxiliary losses (only after warmup)\n",
    "            if not in_warmup:\n",
    "                # Light loss on semantic path\n",
    "                sem_loss = criterion(sem_logits, labels)\n",
    "                # Light loss on GTAL path\n",
    "                gtal_loss = criterion(gtal_logits, labels)\n",
    "                # Soft diversity (only if very imbalanced)\n",
    "                div_loss = soft_diversity_loss(weights)\n",
    "                \n",
    "                loss = main_loss + config.AUXILIARY_WEIGHT * (sem_loss + gtal_loss) + config.DIVERSITY_WEIGHT * div_loss\n",
    "            else:\n",
    "                loss = main_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += main_loss.item()\n",
    "            _, pred = torch.max(ensemble_logits, 1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            epoch_weights.append(weights.detach().cpu().numpy())\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Metrics\n",
    "        train_acc = 100.0 * correct / total\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        all_weights = np.concatenate(epoch_weights, axis=0)\n",
    "        mean_weights = all_weights.mean(axis=0)\n",
    "        history['feature_weights'].append(mean_weights)\n",
    "        \n",
    "        # Evaluate all paths\n",
    "        test_acc, sem_acc, gtal_acc = evaluate_all_paths(model, test_loader)\n",
    "        \n",
    "        # Get ensemble weight\n",
    "        if hasattr(model, 'module'):\n",
    "            alpha = torch.sigmoid(model.module.ensemble_weight).item()\n",
    "        else:\n",
    "            alpha = torch.sigmoid(model.ensemble_weight).item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Record\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "        history['ensemble_alpha'].append(alpha)\n",
    "        history['semantic_acc'].append(sem_acc)\n",
    "        history['gtal_acc'].append(gtal_acc)\n",
    "        \n",
    "        # Save best\n",
    "        is_best = test_acc > best_acc\n",
    "        if is_best:\n",
    "            best_acc = test_acc\n",
    "            best_epoch = epoch + 1\n",
    "            save_dict = {\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "                'history': history,\n",
    "                'config': vars(config)\n",
    "            }\n",
    "            torch.save(save_dict, os.path.join(CHECKPOINT_DIR, 'gtal_plus_v3_best.pth'))\n",
    "        \n",
    "        # Print\n",
    "        marker = 'â˜…' if is_best else ' '\n",
    "        w_str = f\"{mean_weights[0]:.2f}/{mean_weights[1]:.2f}/{mean_weights[2]:.2f}/{mean_weights[3]:.2f}\"\n",
    "        warmup_mark = \"(warmup)\" if in_warmup else \"\"\n",
    "        print(f'{marker}{epoch+1}\\t{avg_loss:.3f}\\t{train_acc:.1f}%\\t{test_acc:.1f}%\\t{sem_acc:.1f}%\\t{gtal_acc:.1f}%\\t{alpha:.2f}\\t{w_str} {warmup_mark}')\n",
    "    \n",
    "    print(\"-\"*85)\n",
    "    print(f\"\\n Training completed!\")\n",
    "    print(f\"   Best Accuracy: {best_acc:.2f}% (Epoch {best_epoch})\")\n",
    "    \n",
    "    # Analysis\n",
    "    print(f\"\\nPath Analysis (final):\")\n",
    "    print(f\"   Semantic path: {history['semantic_acc'][-1]:.2f}%\")\n",
    "    print(f\"   GTAL path: {history['gtal_acc'][-1]:.2f}%\")\n",
    "    print(f\"   Ensemble (Î±={alpha:.2f}): {test_acc:.2f}%\")\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "\n",
    "def evaluate_all_paths(model, loader):\n",
    "    \"\"\"Evaluate ensemble, semantic, and GTAL paths separately\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct_ens = correct_sem = correct_gtal = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            ens_logits, _, sem_logits, gtal_logits = model(imgs, return_both=True)\n",
    "            \n",
    "            _, pred_ens = torch.max(ens_logits, 1)\n",
    "            _, pred_sem = torch.max(sem_logits, 1)\n",
    "            _, pred_gtal = torch.max(gtal_logits, 1)\n",
    "            \n",
    "            correct_ens += (pred_ens == labels).sum().item()\n",
    "            correct_sem += (pred_sem == labels).sum().item()\n",
    "            correct_gtal += (pred_gtal == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return 100.0 * correct_ens / total, 100.0 * correct_sem / total, 100.0 * correct_gtal / total\n",
    "\n",
    "\n",
    "print(\"  - Warmup period for stable early training\")\n",
    "print(\"  - Evaluates all paths separately\")\n",
    "print(\"  - Ensemble combines semantic + GTAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 8. VISUALIZATION FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def visualize_training_results(history, model, test_loader):\n",
    "    \"\"\"Comprehensive visualization of training results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    epochs = range(1, len(history['train_acc']) + 1)\n",
    "    feature_names = ['Early', 'Mid', 'Late', 'Semantic']\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    \n",
    "    # Plot 1: Accuracy curves\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(epochs, history['train_acc'], 'b-', linewidth=2, label='Train', marker='o', markersize=2)\n",
    "    ax1.plot(epochs, history['test_acc'], 'r-', linewidth=2, label='Test', marker='s', markersize=2)\n",
    "    ax1.axhline(y=84, color='g', linestyle='--', linewidth=2, label='Baseline (84%)')\n",
    "    ax1.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "    ax1.set_title('Training & Test Accuracy', fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Loss curve\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(epochs, history['train_loss'], 'b-', linewidth=2, marker='o', markersize=2)\n",
    "    ax2.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax2.set_ylabel('Loss', fontweight='bold')\n",
    "    ax2.set_title('Training Loss', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Feature weights evolution\n",
    "    ax3 = axes[0, 2]\n",
    "    weights_history = np.array(history['feature_weights'])\n",
    "    for i, (name, color) in enumerate(zip(feature_names, colors)):\n",
    "        ax3.plot(epochs, weights_history[:, i], color=color, linewidth=2, label=name, marker='o', markersize=2)\n",
    "    ax3.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax3.set_ylabel('Weight', fontweight='bold')\n",
    "    ax3.set_title('GTAL Feature Weights Evolution', fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Final Nash Equilibrium weights\n",
    "    ax4 = axes[1, 0]\n",
    "    final_weights = weights_history[-1]\n",
    "    bars = ax4.bar(feature_names, final_weights, color=colors, edgecolor='black', linewidth=2)\n",
    "    ax4.set_ylabel('Equilibrium Weight', fontweight='bold')\n",
    "    ax4.set_title('Final Nash Equilibrium Weights', fontweight='bold')\n",
    "    for bar, weight in zip(bars, final_weights):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                f'{weight:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Feature weight distribution on test set\n",
    "    ax5 = axes[1, 1]\n",
    "    model.eval()\n",
    "    all_weights = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            _, weights = model(imgs)\n",
    "            all_weights.append(weights.cpu().numpy())\n",
    "    all_weights = np.concatenate(all_weights, axis=0)\n",
    "    bp = ax5.boxplot([all_weights[:, i] for i in range(4)], labels=feature_names, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    ax5.set_ylabel('Weight', fontweight='bold')\n",
    "    ax5.set_title('Feature Weight Distribution (Test)', fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Stacked area chart\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.stackplot(epochs, weights_history.T, labels=feature_names, colors=colors, alpha=0.8)\n",
    "    ax6.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax6.set_ylabel('Cumulative Weight', fontweight='bold')\n",
    "    ax6.set_title('Feature Contribution Over Training', fontweight='bold')\n",
    "    ax6.legend(loc='upper right')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, 'gtal_plus_results.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    best_acc = max(history['test_acc'])\n",
    "    best_epoch = history['test_acc'].index(best_acc) + 1\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best Test Accuracy: {best_acc:.2f}% (Epoch {best_epoch})\")\n",
    "    print(f\"\\nFinal Nash Equilibrium Weights:\")\n",
    "    for name, weight in zip(feature_names, final_weights):\n",
    "        bar = 'â–ˆ' * int(weight * 40)\n",
    "        print(f\"  {name:10s}: {weight:.4f} {bar}\")\n",
    "    print(f\"\\nMost Important: {feature_names[np.argmax(final_weights)]}\")\n",
    "    \n",
    "    return all_weights\n",
    "\n",
    "\n",
    "print(\"âœ“ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca05e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 9. CREATE DATALOADERS & START TRAINING\n",
    "# =====================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING FOR TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create DataLoaders (matching resnet_finetune_cub)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,  # Same as resnet_finetune_cub\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n DataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "print(f\"\\n GTAL Game Theory Setup:\")\n",
    "print(f\"  Players: 4 feature levels (Early, Mid, Late, Semantic)\")\n",
    "print(f\"  Strategies: Feature weights (attention)\")\n",
    "print(f\"  Payoffs: Relevance - Redundancy penalty\")\n",
    "print(f\"  Solution: Nash Equilibrium via Best Response\")\n",
    "\n",
    "print(f\"\\n Training Configuration:\")\n",
    "print(f\"  Image Size: {config.IMG_SIZE}Ã—{config.IMG_SIZE}\")\n",
    "print(f\"  Epochs: {config.EPOCHS}\")\n",
    "print(f\"  Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Optimizer: SGD (lr={config.BASE_LR}, momentum={config.MOMENTUM})\")\n",
    "print(f\"  Scheduler: StepLR (step={config.LR_STEP_SIZE}, gamma={config.LR_GAMMA})\")\n",
    "print(f\"  Loss: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 10. TRAIN THE V3 MODEL\n",
    "# =====================================================\n",
    "\n",
    "# Initialize model\n",
    "model = GTALPlusModelV3(num_classes=train_dataset.num_classes).to(device)\n",
    "\n",
    "print(\" Starting GTAL+ V3 Training...\")\n",
    "print(\"   Strategy: Semantic baseline + GTAL enhancement â†’ Ensemble\")\n",
    "print(\"   This ensures we never do worse than baseline!\")\n",
    "\n",
    "# Train\n",
    "history, trained_model = train_gtal_plus_v3(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    epochs=config.EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 11. VISUALIZE V3 RESULTS\n",
    "# =====================================================\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(os.path.join(CHECKPOINT_DIR, 'gtal_plus_v3_best.pth'))\n",
    "if hasattr(trained_model, 'module'):\n",
    "    trained_model.module.load_state_dict(checkpoint['model_state'])\n",
    "else:\n",
    "    trained_model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "epochs = range(1, len(history['train_acc']) + 1)\n",
    "feature_names = ['Early', 'Mid', 'Late', 'Semantic']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "# Plot 1: All accuracies\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(epochs, history['test_acc'], 'b-', lw=2, label='Ensemble (Test)')\n",
    "ax1.plot(epochs, history['semantic_acc'], 'g--', lw=1.5, label='Semantic Path')\n",
    "ax1.plot(epochs, history['gtal_acc'], 'r--', lw=1.5, label='GTAL Path')\n",
    "ax1.axhline(y=84, color='k', linestyle=':', lw=2, label='Baseline (84%)')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('V3: All Paths Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Ensemble weight evolution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epochs, history['ensemble_alpha'], 'purple', lw=2)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Î± (Semantic weight)')\n",
    "ax2.set_title('Learned Ensemble Weight')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Feature weights\n",
    "ax3 = axes[0, 2]\n",
    "weights_history = np.array(history['feature_weights'])\n",
    "for i, (name, color) in enumerate(zip(feature_names, colors)):\n",
    "    ax3.plot(epochs, weights_history[:, i], color=color, lw=2, label=name)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Weight')\n",
    "ax3.set_title('GTAL Feature Weights')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Final weights bar\n",
    "ax4 = axes[1, 0]\n",
    "final_weights = weights_history[-1]\n",
    "bars = ax4.bar(feature_names, final_weights, color=colors, edgecolor='black')\n",
    "for bar, w in zip(bars, final_weights):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height(), f'{w:.3f}', ha='center', va='bottom')\n",
    "ax4.set_ylabel('Weight')\n",
    "ax4.set_title('Final GTAL Weights')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 5: Loss\n",
    "ax5 = axes[1, 1]\n",
    "ax5.plot(epochs, history['train_loss'], 'b-', lw=2)\n",
    "ax5.set_xlabel('Epoch')\n",
    "ax5.set_ylabel('Loss')\n",
    "ax5.set_title('Training Loss')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Improvement analysis\n",
    "ax6 = axes[1, 2]\n",
    "final_sem = history['semantic_acc'][-1]\n",
    "final_gtal = history['gtal_acc'][-1]\n",
    "final_ens = history['test_acc'][-1]\n",
    "best_acc = max(history['test_acc'])\n",
    "\n",
    "x = ['Baseline', 'Semantic\\nPath', 'GTAL\\nPath', 'Ensemble', 'Best']\n",
    "y = [84.0, final_sem, final_gtal, final_ens, best_acc]\n",
    "colors_bar = ['gray', '#4ECDC4', '#FF6B6B', 'purple', 'gold']\n",
    "bars = ax6.bar(x, y, color=colors_bar, edgecolor='black')\n",
    "for bar, val in zip(bars, y):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., bar.get_height(), f'{val:.1f}%', ha='center', va='bottom')\n",
    "ax6.set_ylabel('Accuracy (%)')\n",
    "ax6.set_title('V3 Results Comparison')\n",
    "ax6.axhline(y=84, color='red', linestyle='--', lw=1)\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_DIR, 'gtal_plus_v3_results.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Visualization saved to: {VIZ_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
